{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 以下，引っ張ってきたソースコードから．\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        # params →　ニューラルネットのパラメータを保持する辞書型変数\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # それぞれの勾配を保存する．\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        yt = y - t\n",
    "        dy = yt / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"W1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"b1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"W2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"b2\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要するに，入力層784次元，隠れ層が100次元の2層のニューラルネットワークです．はい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09374681,  0.09498098,  0.10096334,  0.1074735 ,  0.09951024,\n",
       "         0.10187313,  0.1001494 ,  0.10387436,  0.09483882,  0.10258942],\n",
       "       [ 0.09408458,  0.09529654,  0.10097381,  0.10807615,  0.09920759,\n",
       "         0.10185238,  0.09984624,  0.10391709,  0.09475228,  0.10199332],\n",
       "       [ 0.09419853,  0.09543666,  0.10131845,  0.10799299,  0.09901134,\n",
       "         0.10152247,  0.10003791,  0.10360197,  0.09466218,  0.1022175 ],\n",
       "       [ 0.09380152,  0.09551234,  0.10109559,  0.1077896 ,  0.09882755,\n",
       "         0.10191297,  0.0993383 ,  0.1042153 ,  0.09509318,  0.10241365],\n",
       "       [ 0.09400441,  0.09509371,  0.10096684,  0.10780344,  0.09941714,\n",
       "         0.10207561,  0.09961887,  0.10413071,  0.09444875,  0.10244053],\n",
       "       [ 0.09362604,  0.09534267,  0.10104098,  0.10807656,  0.09932347,\n",
       "         0.10189097,  0.10002753,  0.10386696,  0.09449205,  0.10231278],\n",
       "       [ 0.09402748,  0.09509935,  0.10116627,  0.10782789,  0.09926916,\n",
       "         0.10186717,  0.09932044,  0.10412801,  0.0948487 ,  0.10244554],\n",
       "       [ 0.09418162,  0.09523823,  0.10071213,  0.10810237,  0.09916785,\n",
       "         0.10171352,  0.10003947,  0.10375127,  0.09458332,  0.10251022],\n",
       "       [ 0.093976  ,  0.09512212,  0.10094773,  0.10811299,  0.09871332,\n",
       "         0.10208831,  0.09991517,  0.10413641,  0.09471865,  0.10226929],\n",
       "       [ 0.0939544 ,  0.09525631,  0.10117992,  0.10797065,  0.09892477,\n",
       "         0.10197585,  0.09966779,  0.1040109 ,  0.09487163,  0.1021878 ],\n",
       "       [ 0.09405607,  0.09509064,  0.10066004,  0.10808291,  0.09925794,\n",
       "         0.10155761,  0.09992439,  0.10379273,  0.09490657,  0.1026711 ],\n",
       "       [ 0.09390964,  0.09512742,  0.10102256,  0.10779045,  0.09884766,\n",
       "         0.10210145,  0.09969796,  0.10407414,  0.09479773,  0.10263099],\n",
       "       [ 0.09414072,  0.09518253,  0.10096931,  0.10789243,  0.09897345,\n",
       "         0.10177046,  0.09965173,  0.10388149,  0.09466654,  0.10287134],\n",
       "       [ 0.09421208,  0.09493755,  0.10101707,  0.10785874,  0.09925326,\n",
       "         0.10179452,  0.09970304,  0.1041869 ,  0.09477224,  0.10226459],\n",
       "       [ 0.09412909,  0.09519375,  0.10116892,  0.10771864,  0.09911402,\n",
       "         0.10189284,  0.09980675,  0.10381573,  0.09476655,  0.10239371],\n",
       "       [ 0.09377159,  0.09502213,  0.10063874,  0.10807247,  0.09958248,\n",
       "         0.1019457 ,  0.1001504 ,  0.10403572,  0.09436974,  0.10241103],\n",
       "       [ 0.09382677,  0.09523193,  0.10092874,  0.10828243,  0.09927031,\n",
       "         0.10173533,  0.09984187,  0.10393989,  0.09450398,  0.10243876],\n",
       "       [ 0.09398525,  0.09531799,  0.10129844,  0.10774779,  0.09915311,\n",
       "         0.10181554,  0.10007933,  0.10344532,  0.09480804,  0.1023492 ],\n",
       "       [ 0.09379874,  0.09514739,  0.10107517,  0.10829814,  0.09899923,\n",
       "         0.10186437,  0.09972482,  0.10398687,  0.09483571,  0.10226956],\n",
       "       [ 0.09403466,  0.09499007,  0.10100312,  0.10778254,  0.09919912,\n",
       "         0.10201944,  0.09955542,  0.10415906,  0.0947114 ,  0.10254517],\n",
       "       [ 0.09378074,  0.09519455,  0.10108912,  0.10799325,  0.09933097,\n",
       "         0.10159597,  0.09955028,  0.10433035,  0.09498812,  0.10214665],\n",
       "       [ 0.09373953,  0.09525851,  0.10086394,  0.10788119,  0.09942047,\n",
       "         0.10184634,  0.09973944,  0.10411681,  0.09470732,  0.10242646],\n",
       "       [ 0.0942469 ,  0.09511599,  0.1010902 ,  0.10755413,  0.0989897 ,\n",
       "         0.10162842,  0.09964852,  0.1042214 ,  0.09514317,  0.10236157],\n",
       "       [ 0.0939218 ,  0.09490951,  0.10074192,  0.10794051,  0.09937431,\n",
       "         0.10172058,  0.09954572,  0.10421962,  0.09506444,  0.10256159],\n",
       "       [ 0.09403932,  0.09498612,  0.10110936,  0.10786996,  0.09938199,\n",
       "         0.10176708,  0.09984426,  0.10376614,  0.09455839,  0.10267738],\n",
       "       [ 0.09397431,  0.09543968,  0.10121475,  0.10803581,  0.09885141,\n",
       "         0.10180157,  0.09972363,  0.10404508,  0.09477931,  0.10213446],\n",
       "       [ 0.09449439,  0.0951997 ,  0.10106224,  0.1076865 ,  0.09901036,\n",
       "         0.10173578,  0.09955743,  0.10421218,  0.09493771,  0.10210373],\n",
       "       [ 0.09393604,  0.09536682,  0.10111145,  0.10801302,  0.09876197,\n",
       "         0.10223168,  0.09994875,  0.10379612,  0.09458434,  0.1022498 ],\n",
       "       [ 0.09417259,  0.09527493,  0.10120752,  0.1080932 ,  0.09887555,\n",
       "         0.10181789,  0.09978483,  0.10382892,  0.09461389,  0.10233067],\n",
       "       [ 0.09362952,  0.094931  ,  0.1009928 ,  0.10781194,  0.09940831,\n",
       "         0.10178689,  0.09994158,  0.10412397,  0.09497123,  0.10240277],\n",
       "       [ 0.09381259,  0.09500446,  0.10109307,  0.10784307,  0.099312  ,\n",
       "         0.10187685,  0.09962247,  0.10400813,  0.09484985,  0.10257751],\n",
       "       [ 0.09436757,  0.09476745,  0.10113662,  0.10795693,  0.09902668,\n",
       "         0.10163083,  0.09973311,  0.10367057,  0.09507542,  0.10263482],\n",
       "       [ 0.09417895,  0.09530175,  0.10120419,  0.10836879,  0.09869235,\n",
       "         0.10162298,  0.0995669 ,  0.10407445,  0.09482398,  0.10216567],\n",
       "       [ 0.09324398,  0.09530958,  0.10134138,  0.10816248,  0.0989056 ,\n",
       "         0.1021393 ,  0.09952449,  0.10412548,  0.09488784,  0.10235986],\n",
       "       [ 0.09410614,  0.09535026,  0.10118743,  0.10820311,  0.09916319,\n",
       "         0.10144307,  0.09945998,  0.10407836,  0.09475093,  0.10225754],\n",
       "       [ 0.09407641,  0.09491496,  0.10094579,  0.10792382,  0.09941011,\n",
       "         0.10180569,  0.09977759,  0.10390146,  0.09448692,  0.10275725],\n",
       "       [ 0.09387049,  0.09467872,  0.10119164,  0.10825874,  0.09942363,\n",
       "         0.1020224 ,  0.09988377,  0.10411136,  0.09450856,  0.10205068],\n",
       "       [ 0.09387305,  0.09532295,  0.10122174,  0.10765403,  0.09925871,\n",
       "         0.10184779,  0.09948449,  0.10424763,  0.09482367,  0.10226595],\n",
       "       [ 0.09387859,  0.09514785,  0.10101422,  0.10774086,  0.09896942,\n",
       "         0.10212892,  0.09996918,  0.1039763 ,  0.09454513,  0.10262952],\n",
       "       [ 0.09387065,  0.09519801,  0.10118841,  0.10818143,  0.09931593,\n",
       "         0.10161356,  0.09960621,  0.10387338,  0.09469001,  0.10246242],\n",
       "       [ 0.09403067,  0.09490947,  0.10100488,  0.10791444,  0.09929861,\n",
       "         0.10219675,  0.09992778,  0.10379272,  0.09450256,  0.10242211],\n",
       "       [ 0.09387782,  0.09538122,  0.10089077,  0.10785303,  0.09901253,\n",
       "         0.10180655,  0.10007287,  0.10370832,  0.09494035,  0.10245655],\n",
       "       [ 0.09401859,  0.09500718,  0.10116489,  0.10784044,  0.09937848,\n",
       "         0.10161621,  0.09967928,  0.10406926,  0.09478241,  0.10244326],\n",
       "       [ 0.09385259,  0.09542558,  0.10095873,  0.1077751 ,  0.09932141,\n",
       "         0.10182807,  0.09956757,  0.10397531,  0.09484497,  0.10245068],\n",
       "       [ 0.09404872,  0.09517193,  0.10094221,  0.10789638,  0.09913456,\n",
       "         0.10168801,  0.10025294,  0.10374027,  0.0946971 ,  0.10242787],\n",
       "       [ 0.09415358,  0.09520229,  0.1007327 ,  0.10782996,  0.0994384 ,\n",
       "         0.10155143,  0.09975288,  0.10394931,  0.09512815,  0.10226131],\n",
       "       [ 0.09413172,  0.09533968,  0.10107531,  0.10808583,  0.09907004,\n",
       "         0.1018884 ,  0.09979542,  0.10365502,  0.09450299,  0.1024556 ],\n",
       "       [ 0.09379741,  0.09534931,  0.10117026,  0.1081334 ,  0.09907804,\n",
       "         0.10222413,  0.09973739,  0.10377256,  0.09460685,  0.10213064],\n",
       "       [ 0.09395017,  0.09498868,  0.10087694,  0.10778835,  0.09926876,\n",
       "         0.10202953,  0.09995418,  0.10399522,  0.09450896,  0.10263922],\n",
       "       [ 0.09376935,  0.0951697 ,  0.10093996,  0.10779419,  0.09933706,\n",
       "         0.1018128 ,  0.09992345,  0.10390389,  0.09480893,  0.10254067],\n",
       "       [ 0.09373184,  0.09483488,  0.10096556,  0.10783922,  0.09919961,\n",
       "         0.10202589,  0.09980993,  0.10447973,  0.09463125,  0.10248209],\n",
       "       [ 0.09402774,  0.09540491,  0.1013306 ,  0.10799566,  0.09885057,\n",
       "         0.10208004,  0.09963601,  0.10366613,  0.09465015,  0.10235818],\n",
       "       [ 0.09397299,  0.09540625,  0.1011998 ,  0.10799086,  0.09879269,\n",
       "         0.10174253,  0.09949468,  0.10375926,  0.09519485,  0.1024461 ],\n",
       "       [ 0.09420019,  0.09504551,  0.10107308,  0.10835938,  0.09891238,\n",
       "         0.10166433,  0.10007387,  0.10380156,  0.09462316,  0.10224654],\n",
       "       [ 0.09423166,  0.09503742,  0.10083966,  0.10750305,  0.09944083,\n",
       "         0.10185335,  0.09996229,  0.10408519,  0.09474019,  0.10230637],\n",
       "       [ 0.0942822 ,  0.09487692,  0.10098042,  0.10755178,  0.09951882,\n",
       "         0.10209517,  0.09976555,  0.10384901,  0.0947625 ,  0.10231762],\n",
       "       [ 0.09411365,  0.09544271,  0.10133807,  0.10787877,  0.09881614,\n",
       "         0.10204522,  0.09977867,  0.10353942,  0.09482816,  0.10221918],\n",
       "       [ 0.09382907,  0.0950828 ,  0.10097238,  0.10814079,  0.09926688,\n",
       "         0.10174042,  0.10006884,  0.10408582,  0.09454466,  0.10226836],\n",
       "       [ 0.09419521,  0.09515182,  0.1012726 ,  0.10803388,  0.0990727 ,\n",
       "         0.10154549,  0.0995751 ,  0.10386438,  0.09479511,  0.10249371],\n",
       "       [ 0.09397206,  0.09517447,  0.10094881,  0.10807544,  0.09896832,\n",
       "         0.10230685,  0.09982129,  0.10367284,  0.09453414,  0.10252578],\n",
       "       [ 0.09392857,  0.09528626,  0.10112304,  0.10820527,  0.09898584,\n",
       "         0.10177996,  0.09984557,  0.10366453,  0.09486419,  0.10231675],\n",
       "       [ 0.09385312,  0.09519109,  0.10099619,  0.1078355 ,  0.09946958,\n",
       "         0.10173836,  0.09999344,  0.10393479,  0.09478848,  0.10219945],\n",
       "       [ 0.09387572,  0.09521461,  0.1009347 ,  0.10817848,  0.09888163,\n",
       "         0.10200166,  0.09966151,  0.10416457,  0.09453738,  0.10254974],\n",
       "       [ 0.09384134,  0.09499264,  0.10137123,  0.10794435,  0.09916908,\n",
       "         0.10173027,  0.09975587,  0.10395831,  0.0948022 ,  0.10243471],\n",
       "       [ 0.09399961,  0.09531914,  0.1011196 ,  0.10791204,  0.09905328,\n",
       "         0.10155664,  0.09966127,  0.10358234,  0.09500082,  0.10279525],\n",
       "       [ 0.09411569,  0.09531488,  0.1007336 ,  0.10831759,  0.09934373,\n",
       "         0.10158048,  0.09983794,  0.10392644,  0.09443476,  0.10239489],\n",
       "       [ 0.09372707,  0.09517703,  0.10100535,  0.10784462,  0.0995399 ,\n",
       "         0.10192225,  0.0997904 ,  0.10412281,  0.09458821,  0.10228235],\n",
       "       [ 0.09423424,  0.09572496,  0.10100677,  0.10799777,  0.09889762,\n",
       "         0.10181426,  0.09945576,  0.10358583,  0.0949049 ,  0.10237788],\n",
       "       [ 0.09363103,  0.0953343 ,  0.10115671,  0.10787021,  0.09920314,\n",
       "         0.10225747,  0.09965874,  0.10351606,  0.09498081,  0.10239153],\n",
       "       [ 0.09384552,  0.09514601,  0.10108783,  0.10775437,  0.09928677,\n",
       "         0.10193814,  0.09973371,  0.10407036,  0.0948807 ,  0.10225658],\n",
       "       [ 0.0940135 ,  0.09501896,  0.10082411,  0.10834432,  0.09936012,\n",
       "         0.10177111,  0.09999584,  0.10357263,  0.09455636,  0.10254305],\n",
       "       [ 0.09375744,  0.09531508,  0.10092407,  0.10779557,  0.09966118,\n",
       "         0.10188302,  0.09970163,  0.10411081,  0.09461087,  0.10224034],\n",
       "       [ 0.09399923,  0.09498674,  0.10096935,  0.10797929,  0.09894995,\n",
       "         0.1014912 ,  0.10038119,  0.10378691,  0.09488717,  0.10256896],\n",
       "       [ 0.09391472,  0.09541193,  0.10119455,  0.10800908,  0.09913011,\n",
       "         0.10182165,  0.09981365,  0.10381638,  0.09467886,  0.10220908],\n",
       "       [ 0.09381719,  0.09527288,  0.10132997,  0.10759328,  0.09926287,\n",
       "         0.10200008,  0.0997626 ,  0.10370622,  0.09484124,  0.10241367],\n",
       "       [ 0.09411651,  0.0951075 ,  0.10098289,  0.10789956,  0.09918067,\n",
       "         0.10187357,  0.0997871 ,  0.10427716,  0.09471788,  0.10205715],\n",
       "       [ 0.094167  ,  0.09496274,  0.10127831,  0.10806666,  0.09914861,\n",
       "         0.10152185,  0.09966666,  0.10367584,  0.09473627,  0.10277606],\n",
       "       [ 0.09417402,  0.09494414,  0.10095829,  0.10797765,  0.09907229,\n",
       "         0.10193663,  0.09961887,  0.10413842,  0.09478347,  0.10239622],\n",
       "       [ 0.09360003,  0.09505889,  0.10115554,  0.10782002,  0.09938826,\n",
       "         0.10205412,  0.09975986,  0.10402395,  0.0946832 ,  0.10245613],\n",
       "       [ 0.09414617,  0.09517917,  0.10121947,  0.10860435,  0.09889364,\n",
       "         0.10159691,  0.09956477,  0.10372432,  0.09460281,  0.10246839],\n",
       "       [ 0.09413359,  0.09498434,  0.10096521,  0.10781188,  0.09899405,\n",
       "         0.10176487,  0.09985957,  0.10433913,  0.09482992,  0.10231744],\n",
       "       [ 0.09386476,  0.09513352,  0.10115321,  0.10783002,  0.09964244,\n",
       "         0.10170766,  0.09957382,  0.10432457,  0.09451153,  0.10225847],\n",
       "       [ 0.09428786,  0.09528227,  0.10109153,  0.10837579,  0.09902324,\n",
       "         0.10150896,  0.09958735,  0.10396639,  0.09450613,  0.10237047],\n",
       "       [ 0.09390853,  0.09551394,  0.10108764,  0.10812591,  0.09903175,\n",
       "         0.10187079,  0.0998805 ,  0.1036722 ,  0.09488826,  0.10202049],\n",
       "       [ 0.09397593,  0.09500632,  0.10104217,  0.10810505,  0.09947053,\n",
       "         0.10198596,  0.09975735,  0.10365245,  0.09477104,  0.1022332 ],\n",
       "       [ 0.09384888,  0.09511546,  0.10113718,  0.10808657,  0.09910297,\n",
       "         0.10175195,  0.09995674,  0.10394738,  0.09468376,  0.10236912],\n",
       "       [ 0.0939928 ,  0.09508643,  0.10136611,  0.10779791,  0.09899243,\n",
       "         0.10194349,  0.09962409,  0.1039848 ,  0.09456244,  0.1026495 ],\n",
       "       [ 0.09406338,  0.09542892,  0.10090439,  0.10816006,  0.0991844 ,\n",
       "         0.10176318,  0.09947323,  0.10368778,  0.09470413,  0.10263053],\n",
       "       [ 0.09416855,  0.09489534,  0.10092446,  0.10794646,  0.09924883,\n",
       "         0.10193268,  0.09976404,  0.10394527,  0.09455406,  0.1026203 ],\n",
       "       [ 0.09367857,  0.09549524,  0.10115041,  0.10773658,  0.09931318,\n",
       "         0.10189889,  0.09980607,  0.10398034,  0.09483214,  0.10210857],\n",
       "       [ 0.09384816,  0.09524626,  0.10105873,  0.10802014,  0.099443  ,\n",
       "         0.10155524,  0.09961073,  0.10366137,  0.09483815,  0.10271823],\n",
       "       [ 0.09393518,  0.09545755,  0.10106739,  0.10773001,  0.09892224,\n",
       "         0.10188317,  0.09941814,  0.10436293,  0.09480209,  0.10242129],\n",
       "       [ 0.09410411,  0.09522456,  0.10079739,  0.10783155,  0.09918602,\n",
       "         0.10181063,  0.09961255,  0.10389716,  0.09497497,  0.10256107],\n",
       "       [ 0.09401877,  0.09524734,  0.10123203,  0.10811159,  0.09868733,\n",
       "         0.10217813,  0.10005041,  0.10373639,  0.09445133,  0.10228668],\n",
       "       [ 0.09396349,  0.095166  ,  0.10075501,  0.10800211,  0.09957215,\n",
       "         0.10172984,  0.09978446,  0.10378163,  0.09490132,  0.10234398],\n",
       "       [ 0.09396725,  0.09484204,  0.10088321,  0.10793189,  0.09955946,\n",
       "         0.10194767,  0.0997647 ,  0.1036542 ,  0.09475396,  0.10269562],\n",
       "       [ 0.09392552,  0.09514152,  0.1009084 ,  0.10791226,  0.09936753,\n",
       "         0.10172753,  0.09981331,  0.10419343,  0.09459936,  0.10241115],\n",
       "       [ 0.09389864,  0.09493465,  0.10090126,  0.10790295,  0.09915385,\n",
       "         0.10211704,  0.10011419,  0.10377357,  0.09460957,  0.10259429],\n",
       "       [ 0.09378936,  0.09515698,  0.10089192,  0.10853145,  0.09893836,\n",
       "         0.10153885,  0.10024737,  0.1039485 ,  0.09461956,  0.10233765],\n",
       "       [ 0.0941159 ,  0.09512958,  0.1010547 ,  0.10735385,  0.09933764,\n",
       "         0.10172521,  0.09981097,  0.10434582,  0.09462829,  0.10249804]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # 100枚の画像（今回はランダムな値）に対してそれぞれ予測したスコアがこちら（もちろん結果も適当）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = net.numerical_gradient(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "iter_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,10) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4759a456b7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-040b5b9b6e10>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,10) (100,) "
     ]
    }
   ],
   "source": [
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "なんかコケる……．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
